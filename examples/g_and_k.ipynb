{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.models.g_and_k_model import uni_g_and_k\n",
    "from src.scoring_rules.scoring_rules import EnergyScore\n",
    "from src.transformers import BoundedVarTransformer\n",
    "from src.sampler.sgMCMC import SGMCMC\n",
    "import torch\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake observation data\n",
    "gk = uni_g_and_k()\n",
    "obs = gk.torch_forward_simulate(torch.tensor([2.0, 2.0, 2.0,2.0]), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We assume a simple Lorenz 96 Model with three parameters. These parameters are defined with a uniform prior of [1.4,2.2]x[0,1]x[1.5,2.5]. \n",
    "\n",
    "# Usage\n",
    "We need to define a `joint_log_prob` function which calculates the log unnormalised density, otherwise known as the potential function. ($U(\\theta)$)\n",
    "$$\n",
    "\\pi(\\theta) \\sim exp(-U(\\theta))\n",
    "$$\n",
    "# Notes\n",
    "1. SGMCMC is defined only on an unconstrained space. Since our parameters are defined on a constrained space, a (logit) transformation is required. This transformation is done in the `joint_log_prob` function. The jacobian adjustment term is added in here as well.\n",
    "2. By default, the `SGMCMC` class does SG-NHT algorithm for sampling.\n",
    "3. The `SGMCMC` class uses an optimisation step (with ADAM optimisation) to first find the initial parameters. Then, to find the optimal step size, an multi-armed bandit algorithm is utilised (this can be turned off by setting `use_mamba=False` in the `sampler.sample()` function, although then a specific step size should be passed into the function). Finally, the SGMCMC algorithm is ran.\n",
    "4. The `sampler.sample()` function returns a dictionary with useful information about the inference procedure that was ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:01<00:00, 150.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial params: [ 0.35543389 -0.5664922   0.78635375  0.4932961 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e1d82fc15f413abfed9d03322fd9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transformer for parameters\n",
    "lower_limit_arr = np.array([0,0,0,0])\n",
    "upper_limit_arr = np.array([4,4,4,4])\n",
    "bounded_trans = BoundedVarTransformer(lower_bound=lower_limit_arr, upper_bound=upper_limit_arr)\n",
    "\n",
    "# Scoring Rule\n",
    "es = EnergyScore(weight=1)\n",
    "\n",
    "# Model\n",
    "gk = uni_g_and_k()\n",
    "\n",
    "# Build joint log prob function\n",
    "def joint_log_prob(param_torch_unconstrained, obs, model, scoring_rule, transformer, n_samples_per_param=500):\n",
    "    # 1. Transform to constrained space\n",
    "    # 2. Calculate LAJ (unconstrained)\n",
    "    # 3. Calculate log prior (constrained)\n",
    "    # 4. Calculate log likelihood / score (constrained) (weights should be init in the scoring rule class)\n",
    "\n",
    "    param_torch_constrained = transformer.inverse_transform(param_torch_unconstrained, use_torch=True)\n",
    "\n",
    "    # LAJ\n",
    "    laj = transformer.jac_log_det_inverse_transform(param_torch_unconstrained, use_torch=True)\n",
    "\n",
    "    # Log prior\n",
    "    log_prior = 0 # Uniform prior is assumed\n",
    "\n",
    "    # Log likelihood    \n",
    "    sims = model.torch_forward_simulate(param_torch_constrained, n_samples_per_param)\n",
    "    log_ll = scoring_rule.loglikelihood(y_obs = obs, y_sim = sims, use_torch=True)\n",
    "    model.scores.append(log_ll.detach()) #Detach otherwise mem explodes!\n",
    "    return laj + log_prior + log_ll\n",
    "\n",
    "\n",
    "joint_log_prob_func = functools.partial(joint_log_prob, model=gk, scoring_rule=es, transformer=bounded_trans)\n",
    "\n",
    "# Sampler\n",
    "sampler = SGMCMC(gk, observations=obs, joint_log_prob=joint_log_prob_func, transformer=bounded_trans, n_samples=5000)\n",
    "\n",
    "op = sampler.sample(use_mamba=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py3.8_sigkernel_1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "213c99d5eca6d51a3e18768ad81a460afa8debcd534e14665bf23564017a28cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
